####ç¬¬ä¸€é¢˜################################################################################
# 1. å®šä¹‰ä¸€ä¸ªç±»TextAnalyzerï¼Œå…¶å±æ€§åŒ…æ‹¬å¾…åˆ†æçš„æ–‡æœ¬æ–‡ä»¶è·¯å¾„ï¼Œç­‰åŠ è½½çš„é¢„è®­ç»ƒæ¨¡å‹æ–‡ä»¶è·¯å¾„ï¼Œ       #
# è®­ç»ƒword2vecçš„ä¸€äº›ç®€å•å‚æ•°ï¼ˆå¦‚å‘é‡é•¿åº¦ï¼Œçª—å£å¤§å°ï¼‰ç­‰ï¼Œåˆå§‹åŒ–çš„æ—¶å€™éœ€è¦å¯¹è¿™äº›å±æ€§è¿›è¡Œå®šä¹‰ã€‚     #
##########################################################################################
import pandas as pd
import numpy as np
import jieba
from gensim.models import Word2Vec
from tqdm import tqdm
class TextAnalyzer:
    def __init__(self,file_path,model_path,len_vec,size_window):
        self.file_path = file_path
        self.model_path = model_path
        self.len_vec = len_vec
        self.size_window = size_window
        
        self.processed_words = None # å¢åŠ ç¼“å­˜
    def _pre_process(self):
        if self.processed_words:
            return self.processed_words
        
        # ä½¿ç”¨jiebaå¤šçº¿ç¨‹æ¨¡å¼
        # jieba.enable_parallel(4) 
        stopset = ([
            'ï¼Œ','ã€','ã€‘','#','s','1','2','3','4','5','6','7','8','9','MIUI','@','_','(',')','ï¿£','ï¼','[',']','-',
            'â€œ','â€','p','ID',':','ï¼š','~','td','t','d','//','/','ï¼Ÿ','ã€‚','%','>>','>','ã€Š','ã€‹','ã€','+','ing','TNND','ã€œ',
            'ğŸ‚','âŠ™','o','(âŠ™oâŠ™)','âŠ™oâŠ™','.','..','..........','â„ƒ','H','â€¦â€¦','^','*','$','ï¿¥','â†—','â†–','Ï‰','â†–(^Ï‰^)â†—','ğŸ˜ƒ','â€¦','""','"','Y',
        ])
        chunk_size = 10000
        chunks = pd.read_csv(# csvæ›´å¿«
            self.file_path,
            sep = '\t',
            header=0,# ç¬¬ä¸€è¡Œæ˜¯åˆ—åï¼ˆlocation/text/user_id/weibo_created_atï¼‰
            names=['text'],         # æŒ‡å®šåˆ—åï¼ˆè¦†ç›–åŸæœ‰åˆ—åï¼‰
            chunksize=chunk_size,
            usecols=[1],# åªè¯»å–å†…å®¹åˆ—
            dtype={'text':str},
            engine='c',
            on_bad_lines='skip'     # è·³è¿‡æ ¼å¼é”™è¯¯è¡Œ
            )
        
        words = []
        #jieba.enable_parallel(4)  # å¼€å¯4ä¸ªçº¿ç¨‹æé«˜é€Ÿåº¦
        for chunk in tqdm(chunks, desc="Processing text chunks",dynamic_ncols=True):
            texts = chunk['text'].dropna().unique()
            words.extend([[w for w in jieba.lcut(text)if w not in stopset]
                         for text in texts])
            
        self.processed_words = words
        return words
    def _get_word2vec_model(self):
        words = self._pre_process()
        model = Word2Vec(words,
                        vector_size = self.len_vec,
                        window = self.size_window,
                        min_count = 2,
                        workers=4,
                        sg=1,             # ä½¿ç”¨ Skip-gram ç®—æ³•
                        alpha=0.01,       # åˆå§‹å­¦ä¹ ç‡
 #                       min_alpha=0.0001, # æœ€å°å­¦ä¹ ç‡
#                        seed=42,          # éšæœºç§å­
                        sample=1e-5,      # é«˜é¢‘è¯ä¸‹é‡‡æ ·é˜ˆå€¼
                        hs=0,             # ä½¿ç”¨è´Ÿé‡‡æ ·
                        negative=10,      # è´Ÿé‡‡æ ·æ•°é‡
                        epochs=20         # è¿­ä»£æ¬¡æ•°
                        
                        )
        model.save(self.model_path)
        print("model saved")
        return model
    def get_similar_words(self, string):
        model = self._get_word2vec_model()
        most_similar = model.wv.most_similar(string,topn = 10)
        #least_similar = model.wv.most_similar(negative = str(string), topn = 10)
        return most_similar
        
if __name__ == "__main__":
    text_ana = TextAnalyzer(
        file_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/weibo.txt',
        model_path='/Users/wtsama/Documents/code/code2/python_assignment/week5/word2vec.model',
        len_vec=300,
        size_window=10
    )
    print(text_ana.get_similar_words("å›å¿†"))
####ç¬¬äºŒé¢˜##########################################################################################
# åœ¨ä¸Šè¿°ç±»åŠ å…¥ä¸€ä¸ªé¢„å¤„ç†æ–¹æ³•_pre_processï¼Œå¦‚å°†å¾…åˆ†æçš„weibo.txtåŠ è½½åˆ°å†…å­˜ï¼ˆè¯·å…ˆè§£å‹æä¾›çš„weibo.txt.zip)ï¼Œ#
# è¿›è¡ŒåŸºæœ¬çš„æ–‡æœ¬é¢„å¤„ç†ï¼Œå¦‚å¯¹æ‰€æœ‰å¾®åšå†…å®¹è¿›è¡Œå»é‡ï¼Œè¿›è¡Œåˆ†è¯ã€å»é™¤åœç”¨è¯ã€æ ‡ç‚¹ç­‰ï¼Œ                          #
# æœ€ç»ˆå»ºç«‹ä¸€ä¸ªä»¥å¾®åšä¸ºå•ä½è¿›è¡Œåˆ†è¯çš„äºŒç»´åˆ—è¡¨ã€‚æ³¨æ„ï¼Œweibo.txtä¸€è¡Œä¸ºä¸€æ¡å¾®åšçš„å±æ€§ï¼Œ                       #
# ç”¨\tåˆ†éš”åï¼Œç¬¬äºŒä¸ªå…ƒç´ ä¸ºå¾®åšå†…å®¹ã€‚ï¼ˆæä¾›çš„weibo.txtåŒ…å«å¤§é‡é‡å¤å’Œæ ‡ç‚¹ç­‰ï¼Œéœ€è¦ä»”ç»†é¢„å¤„ç†ï¼Œ                #
# å¦åˆ™ä¼šå½±å“åé¢çš„åµŒå…¥æ¨¡å‹è®­ç»ƒã€‚ï¼‰                                                                     #
#####################################################################################################
